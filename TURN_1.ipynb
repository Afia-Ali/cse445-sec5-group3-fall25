{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzVnSRvARj56ybGLhSbkCR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Afia-Ali/cse445-sec5-group3-fall25/blob/Abir/TURN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4zrXlwbDRGku",
        "outputId": "774a3913-e21e-4997-fbf3-402c756bf57b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9a59ef9c-f883-49b0-ac6d-74fcaa0a4b3d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9a59ef9c-f883-49b0-ac6d-74fcaa0a4b3d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Dataset.v1i.yolov8.zip to Dataset.v1i.yolov8 (1).zip\n",
            "Dataset path detected: dataset/Dataset.v1i.yolov8\n",
            "Folders inside dataset: ['valid', 'train', 'data.yaml', 'test', 'README.dataset.txt', 'README.roboflow.txt']\n",
            "Train images: 58\n",
            "Validation images: 12\n",
            "Test images: 9\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 75.0MB/s 0.1s\n",
            "Ultralytics 8.3.233 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/Dataset.v1i.yolov8/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 16.4MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1728.0Â±642.0 MB/s, size: 101.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/Dataset.v1i.yolov8/train/labels... 58 images, 0 backgrounds, 23 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 58/58 758.6it/s 0.1s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0160_MP4-10_jpg.rf.374685ec8ec1d91fefd63bb647da5eec.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0160_MP4-11_jpg.rf.28f0f55bcef85f6a6d10439094eff1a2.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0160_MP4-14_jpg.rf.db65c20e5b3bcc8fef61c3b65bdfaad6.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0160_MP4-16_jpg.rf.9e8845c4aa22a15380c6645dd0b846d0.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0160_MP4-30_jpg.rf.6d7fa29461ff338bc9897ef478ac65c7.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0160_MP4-3_jpg.rf.2415a82f460fea4c6589735c4268604f.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0160_MP4-6_jpg.rf.a4b958e93c457d074c005f78057cbdaf.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0234_MP4-0_jpg.rf.60f388a68241cb86d67883542c67e95f.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0234_MP4-12_jpg.rf.37463fa9135dc75a06cd5a00d7e6fde2.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0234_MP4-14_jpg.rf.f4b1a0d2108973f01dc6118595ef8126.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0234_MP4-15_jpg.rf.146cb1d35d96f72d5f16564594e0745e.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0234_MP4-1_jpg.rf.83246db1ae5ff91d7b3df70e375ff593.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0234_MP4-20_jpg.rf.31ec3881a8d5ef6352f6a5f0e09a6e22.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0234_MP4-7_jpg.rf.7e89436fe8628739cfe8d27fcadc8fe7.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0661_MP4-0_jpg.rf.93639bea0c275991f4fe332b7b1d8c02.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0661_MP4-10_jpg.rf.7d3cfbc952947a92fb6f6080ab56479a.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0661_MP4-11_jpg.rf.c32cecb9c236b3325e749e1a54c17df9.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0661_MP4-12_jpg.rf.e8b1e6fb80d1905187127b3e15ca39f4.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0661_MP4-31_jpg.rf.eab477188911d4c657d479a0a1660f90.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0661_MP4-32_jpg.rf.503975c8952a75b0736d3974cfac895e.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0661_MP4-3_jpg.rf.816ce06aaff4e8cde911cb99392cc5df.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/GRMN0661_MP4-7_jpg.rf.f9c82128f499d8aa279e753de8aaa66d.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/dataset/Dataset.v1i.yolov8/train/images/road-220058_1280_jpg.rf.22d7223047a2dc715a05a59ed2af96b6.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset/Dataset.v1i.yolov8/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1281.6Â±528.0 MB/s, size: 73.9 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/Dataset.v1i.yolov8/valid/labels... 12 images, 0 backgrounds, 6 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 12/12 1.5Kit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/dataset/Dataset.v1i.yolov8/valid/images/GRMN0160_MP4-20_jpg.rf.50955176364ca017ce2f34b7f854183d.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/dataset/Dataset.v1i.yolov8/valid/images/GRMN0160_MP4-32_jpg.rf.af647191feb7d389595088f5dcac0a25.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/dataset/Dataset.v1i.yolov8/valid/images/GRMN0234_MP4-18_jpg.rf.e9a1a5a3cce63c5ff6a309bce4a2b7c3.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/dataset/Dataset.v1i.yolov8/valid/images/GRMN0234_MP4-5_jpg.rf.4842c075a0fc03ee0d915f61096d7f51.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/dataset/Dataset.v1i.yolov8/valid/images/GRMN0661_MP4-13_jpg.rf.479814e12ad6ea5d64d9dd0288913a1f.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/dataset/Dataset.v1i.yolov8/valid/images/GRMN0661_MP4-33_jpg.rf.3974d9910fb5e954372da439aff0ae4f.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset/Dataset.v1i.yolov8/valid/labels.cache\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50         0G      1.796      3.459      1.783          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.5s/it 32.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6    0.00333          1      0.719      0.441\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50         0G      1.122      2.476      1.232         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6    0.00333          1      0.995      0.712\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50         0G     0.9816      1.573       1.13          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 30.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.4s/it 1.4s\n",
            "                   all          6          6     0.0166          1      0.995      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50         0G     0.8289      1.271      1.043         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6          1      0.988      0.995      0.794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50         0G     0.9118      1.247      1.065         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.7s/it 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6          1      0.896      0.995      0.826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50         0G     0.9094      1.316      1.057          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6          1      0.886      0.995      0.805\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50         0G     0.7741      1.087     0.9567         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.4s/it 32.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.957          1      0.995      0.794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50         0G     0.8553      1.093      1.032          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.975          1      0.995      0.832\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50         0G     0.8829      1.135       1.04          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1s/it 1.1s\n",
            "                   all          6          6       0.99          1      0.995      0.758\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50         0G     0.7969      1.269       1.01          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 29.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.5s/it 1.5s\n",
            "                   all          6          6       0.99          1      0.995      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50         0G      1.032      1.165      1.049          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 30.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6          1       0.99      0.995      0.758\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50         0G     0.8582      1.149      1.045          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 28.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6          1      0.992      0.995      0.846\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50         0G     0.9212      1.153      1.063         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6          1      0.992      0.995      0.811\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50         0G      0.829      1.138     0.9835         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 30.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6          1      0.992      0.995      0.811\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50         0G     0.8578      1.124      1.054          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 30.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1s/it 1.1s\n",
            "                   all          6          6          1      0.998      0.995       0.73\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50         0G     0.8717      1.136      1.031         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.995          1      0.995       0.83\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50         0G     0.8807      1.203      1.052          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 28.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.841\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50         0G     0.8835      1.211      1.082          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.841\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50         0G     0.8195       1.09      1.008         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.992          1      0.995       0.84\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50         0G     0.8554      1.136      1.037          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.4s/it 1.4s\n",
            "                   all          6          6      0.989          1      0.995      0.839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50         0G     0.8499      1.036      1.058          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 28.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.989          1      0.995      0.839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50         0G     0.8489      1.094      1.041          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 30.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.991          1      0.995      0.875\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50         0G     0.7455     0.9433      1.017          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.832\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50         0G     0.8471     0.9698      1.062          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 28.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.832\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50         0G     0.7067     0.8943     0.9772          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.5s/it 1.5s\n",
            "                   all          6          6      0.993          1      0.995       0.79\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50         0G     0.7674      1.047      1.022          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.6s/it 33.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.993          1      0.995       0.79\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50         0G     0.7904      1.025     0.9894          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.1s/it 30.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.3s/it 1.3s\n",
            "                   all          6          6      0.989          1      0.995       0.88\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50         0G     0.8753     0.9612      1.001          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.856\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50         0G     0.7374     0.9014     0.9769          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 29.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.856\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50         0G     0.7249     0.9589     0.9979          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.871\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50         0G     0.7668     0.9597     0.9956          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.869\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50         0G     0.8876      1.083      1.037          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.4s/it 1.4s\n",
            "                   all          6          6      0.992          1      0.995      0.869\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50         0G     0.8616     0.9612      1.013          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.786\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50         0G      0.711     0.8622     0.9942          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.7s/it 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.786\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50         0G     0.7617     0.9078      1.011          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 30.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.864\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50         0G     0.7186     0.9452     0.9748          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.895\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50         0G     0.6838     0.8376     0.9387          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.2s/it 30.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.5s/it 1.5s\n",
            "                   all          6          6      0.992          1      0.995      0.895\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50         0G     0.7119     0.8012     0.9432          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.991          1      0.995      0.888\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50         0G     0.6772     0.8259     0.9483          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.7s/it 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6       0.99          1      0.995      0.845\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50         0G     0.7153     0.8325     0.9751         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 6.0s/it 29.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6       0.99          1      0.995      0.845\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50         0G     0.6966      1.155      1.062          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.845\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50         0G     0.6026      1.134     0.9406          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 28.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.3s/it 1.3s\n",
            "                   all          6          6      0.992          1      0.995      0.845\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50         0G     0.5775      1.134     0.9152          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 28.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.848\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50         0G      0.553     0.9865     0.8655          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.848\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50         0G      0.554      1.003      0.867          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.5s/it 1.5s\n",
            "                   all          6          6      0.992          1      0.995      0.848\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50         0G     0.5488      1.033     0.9183          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.825\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50         0G     0.5655      1.017     0.9193          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.7s/it 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.852\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50         0G     0.5201       1.01     0.8936          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.9s/it 29.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0s/it 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.852\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50         0G     0.5332     0.9829      0.883          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 29.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
            "                   all          6          6      0.992          1      0.995      0.852\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50         0G     0.6045       1.04     0.9919          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.8s/it 28.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.3s/it 1.3s\n",
            "                   all          6          6      0.992          1      0.995      0.852\n",
            "\n",
            "50 epochs completed in 0.430 hours.\n",
            "Optimizer stripped from /content/runs/detect/train/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/train/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.233 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.3s/it 1.3s\n",
            "                   all          6          6      0.992          1      0.995      0.895\n",
            "Speed: 1.5ms preprocess, 183.9ms inference, 0.0ms loss, 6.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "\n",
            "image 1/1 /content/dataset/Dataset.v1i.yolov8/test/images/GRMN0661_MP4-9_jpg.rf.337103d0d96d90d120a606e39cf7d502.jpg: 384x640 1 road-lanes, 165.3ms\n",
            "Speed: 4.3ms preprocess, 165.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'show'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3394700907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mtest_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dataset_path}/test/images/{os.listdir(dataset_path+'/test/images')[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Turn prediction logic for a single image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'show'"
          ]
        }
      ],
      "source": [
        "# ------------------------------\n",
        "# STEP 0: Install YOLOv8\n",
        "# ------------------------------\n",
        "!pip install ultralytics --quiet\n",
        "\n",
        "# ------------------------------\n",
        "# STEP 1: Upload & Unzip Dataset\n",
        "# ------------------------------\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload your dataset zip\n",
        "uploaded = files.upload()  # select Dataset.v1i.yolov8.zip\n",
        "\n",
        "# Unzip into /dataset\n",
        "zip_filename = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset\")\n",
        "\n",
        "# Detect top-level folder inside dataset\n",
        "top_level_folders = [f for f in os.listdir(\"dataset\")\n",
        "                     if os.path.isdir(os.path.join(\"dataset\", f))]\n",
        "\n",
        "dataset_path = os.path.join(\"dataset\", top_level_folders[0])\n",
        "print(\"Dataset path detected:\", dataset_path)\n",
        "print(\"Folders inside dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Check images\n",
        "print(\"Train images:\", len(os.listdir(f\"{dataset_path}/train/images\")))\n",
        "print(\"Validation images:\", len(os.listdir(f\"{dataset_path}/valid/images\")))\n",
        "print(\"Test images:\", len(os.listdir(f\"{dataset_path}/test/images\")))\n",
        "\n",
        "# ------------------------------\n",
        "# STEP 2: Train YOLOv8 Detection Model\n",
        "# ------------------------------\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "model.train(\n",
        "    data=f\"{dataset_path}/data.yaml\",  # correct\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=8,\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# STEP 3: Test Detection on a Single Image\n",
        "# ------------------------------\n",
        "import cv2\n",
        "\n",
        "# Pick a test image\n",
        "test_img = f\"{dataset_path}/test/images/{os.listdir(dataset_path+'/test/images')[0]}\"\n",
        "results = model.predict(test_img)\n",
        "results.show()\n",
        "\n",
        "# Turn prediction logic for a single image\n",
        "img = cv2.imread(test_img)\n",
        "h, w, _ = img.shape\n",
        "\n",
        "# Get bounding boxes (x_min, y_min, x_max, y_max)\n",
        "boxes = results[0].boxes.xyxy.cpu().numpy() if len(results[0].boxes) > 0 else []\n",
        "\n",
        "if len(boxes) > 0:\n",
        "    lane_centers = (boxes[:,0] + boxes[:,2]) / 2\n",
        "    avg_lane_center = lane_centers.mean()\n",
        "    if avg_lane_center < w/3:\n",
        "        turn = \"LEFT\"\n",
        "    elif avg_lane_center > 2*w/3:\n",
        "        turn = \"RIGHT\"\n",
        "    else:\n",
        "        turn = \"STRAIGHT\"\n",
        "else:\n",
        "    turn = \"STRAIGHT\"\n",
        "\n",
        "print(\"Predicted turn:\", turn)\n",
        "\n",
        "# ------------------------------\n",
        "# STEP 4: Predict Turns on a Video\n",
        "# ------------------------------\n",
        "# Upload your video if you want to test\n",
        "uploaded_videos = files.upload()  # e.g., test_video.mp4\n",
        "video_filename = list(uploaded_videos.keys())[0]\n",
        "\n",
        "cap = cv2.VideoCapture(video_filename)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Predict lanes\n",
        "    results = model.predict(frame)\n",
        "    boxes = results[0].boxes.xyxy.cpu().numpy() if len(results[0].boxes) > 0 else []\n",
        "    h, w, _ = frame.shape\n",
        "\n",
        "    # Turn logic\n",
        "    if len(boxes) > 0:\n",
        "        lane_centers = (boxes[:,0] + boxes[:,2]) / 2\n",
        "        avg_lane_center = lane_centers.mean()\n",
        "        if avg_lane_center < w/3:\n",
        "            turn = \"LEFT\"\n",
        "        elif avg_lane_center > 2*w/3:\n",
        "            turn = \"RIGHT\"\n",
        "        else:\n",
        "            turn = \"STRAIGHT\"\n",
        "    else:\n",
        "        turn = \"STRAIGHT\"\n",
        "\n",
        "    # Display turn on frame\n",
        "    cv2.putText(frame, f\"Turn: {turn}\", (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
        "    cv2.imshow(\"Lane Prediction\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# ------------------------------\n",
        "# STEP 5: Save Model for Deployment\n",
        "# ------------------------------\n",
        "model.export(format=\"onnx\")  # for deployment\n",
        "model.save(\"road_lane_model.pt\")"
      ]
    }
  ]
}